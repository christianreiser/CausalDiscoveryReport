
@article{peters_identifiability_2014,
	title = {Identifiability of {Gaussian} structural equation models with equal error variances},
	volume = {101},
	issn = {1464-3510, 0006-3444},
	url = {http://arxiv.org/abs/1205.2536},
	doi = {10.1093/biomet/ast043},
	abstract = {We consider structural equation models in which variables can be written as a function of their parents and noise terms, which are assumed to be jointly independent. Corresponding to each structural equation model, there is a directed acyclic graph describing the relationships between the variables. In Gaussian structural equation models with linear functions, the graph can be identified from the joint distribution only up to Markov equivalence classes, assuming faithfulness. In this work, we prove full identifiability if all noise variables have the same variances: the directed acyclic graph can be recovered from the joint Gaussian distribution. Our result has direct implications for causal inference: if the data follow a Gaussian structural equation model with equal error variances and assuming that all variables are observed, the causal structure can be inferred from observational data only. We propose a statistical method and an algorithm that exploit our theoretical findings.},
	number = {1},
	urldate = {2022-02-10},
	journal = {Biometrika},
	author = {Peters, Jonas and Bühlmann, Peter},
	month = mar,
	year = {2014},
	note = {arXiv: 1205.2536},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning},
	pages = {219--228},
}

@article{zhang_causal_nodate,
	title = {Causal {Reasoning} with {Ancestral} {Graphs}},
	abstract = {Causal reasoning is primarily concerned with what would happen to a system under external interventions. In particular, we are often interested in predicting the probability distribution of some random variables that would result if some other variables were forced to take certain values. One prominent approach to tackling this problem is based on causal Bayesian networks, using directed acyclic graphs as causal diagrams to relate post-intervention probabilities to pre-intervention probabilities that are estimable from observational data. However, such causal diagrams are seldom fully testable given observational data. In consequence, many causal discovery algorithms based on data-mining can only output an equivalence class of causal diagrams (rather than a single one). This paper is concerned with causal reasoning given an equivalence class of causal diagrams, represented by a (partial) ancestral graph. We present two main results. The ﬁrst result extends Pearl (1995)’s celebrated do-calculus to the context of ancestral graphs. In the second result, we focus on a key component of Pearl’s calculus—the property of invariance under interventions, and give stronger graphical conditions for this property than those implied by the ﬁrst result. The second result also improves the earlier, similar results due to Spirtes et al. (1993).},
	language = {en},
	author = {Zhang, Jiji},
	pages = {38},
}

@article{zhang_causal_nodate-1,
	title = {Causal {Reasoning} with {Ancestral} {Graphs}},
	abstract = {Causal reasoning is primarily concerned with what would happen to a system under external interventions. In particular, we are often interested in predicting the probability distribution of some random variables that would result if some other variables were forced to take certain values. One prominent approach to tackling this problem is based on causal Bayesian networks, using directed acyclic graphs as causal diagrams to relate post-intervention probabilities to pre-intervention probabilities that are estimable from observational data. However, such causal diagrams are seldom fully testable given observational data. In consequence, many causal discovery algorithms based on data-mining can only output an equivalence class of causal diagrams (rather than a single one). This paper is concerned with causal reasoning given an equivalence class of causal diagrams, represented by a (partial) ancestral graph. We present two main results. The ﬁrst result extends Pearl (1995)’s celebrated do-calculus to the context of ancestral graphs. In the second result, we focus on a key component of Pearl’s calculus—the property of invariance under interventions, and give stronger graphical conditions for this property than those implied by the ﬁrst result. The second result also improves the earlier, similar results due to Spirtes et al. (1993).},
	language = {en},
	author = {Zhang, Jiji},
	pages = {38},
}

@article{colombo_order-independent_nodate,
	title = {Order-{Independent} {Constraint}-{Based} {Causal} {Structure} {Learning}},
	abstract = {We consider constraint-based methods for causal structure learning, such as the PC-, FCI-, RFCI- and CCD- algorithms (Spirtes et al., 1993, 2000; Richardson, 1996; Colombo et al., 2012; Claassen et al., 2013). The ﬁrst step of all these algorithms consists of the adjacency search of the PC-algorithm. The PC-algorithm is known to be order-dependent, in the sense that the output can depend on the order in which the variables are given. This order-dependence is a minor issue in low-dimensional settings. We show, however, that it can be very pronounced in high-dimensional settings, where it can lead to highly variable results. We propose several modiﬁcations of the PC-algorithm (and hence also of the other algorithms) that remove part or all of this order-dependence. All proposed modiﬁcations are consistent in high-dimensional settings under the same conditions as their original counterparts. We compare the PC-, FCI-, and RFCI-algorithms and their modiﬁcations in simulation studies and on a yeast gene expression data set. We show that our modiﬁcations yield similar performance in low-dimensional settings and improved performance in high-dimensional settings. All software is implemented in the R-package pcalg.},
	language = {en},
	author = {Colombo, Diego and Maathuis, Marloes H},
	pages = {42},
}

@article{runge_pcmci_2019,
	title = {{PCMCI} - {Detecting} and quantifying causal associations in large nonlinear time series datasets},
	volume = {5},
	url = {https://www.science.org/doi/10.1126/sciadv.aau4996},
	doi = {10.1126/sciadv.aau4996},
	number = {11},
	urldate = {2021-12-08},
	journal = {Science Advances},
	author = {Runge, Jakob and Nowack, Peer and Kretschmer, Marlene and Flaxman, Seth and Sejdinovic, Dino},
	year = {2019},
	note = {Publisher: American Association for the Advancement of Science},
	keywords = {Physics - Atmospheric and Oceanic Physics, Statistics - Applications, Statistics - Methodology},
	pages = {15},
}

@article{gerhardus_characterization_2021,
	title = {Characterization of causal ancestral graphs for time series with latent confounders},
	url = {http://arxiv.org/abs/2112.08417},
	abstract = {Generalizing directed maximal ancestral graphs, we introduce a class of graphical models for representing time lag specific causal relationships and independencies among finitely many regularly sampled and regularly subsampled time steps of multivariate time series with unobserved variables. We completely characterize these graphs and show that they entail constraints beyond those that have previously been considered in the literature. This allows for stronger causal inferences without having imposed additional assumptions. In generalization of directed partial ancestral graphs we further introduce a graphical representation of Markov equivalence classes of the novel type of graphs and show that these are more informative than what current state-of-the-art causal discovery algorithms learn. We also analyze the additional information gained by increasing the number of observed time steps.},
	urldate = {2022-01-28},
	journal = {arXiv:2112.08417 [cs, stat]},
	author = {Gerhardus, Andreas},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.08417},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@article{richardson_ancestral_2002,
	title = {Ancestral graph {Markov} models},
	volume = {30},
	issn = {0090-5364},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-30/issue-4/Ancestral-graph-Markov-models/10.1214/aos/1031689015.full},
	doi = {10.1214/aos/1031689015},
	language = {en},
	number = {4},
	urldate = {2022-01-31},
	journal = {The Annals of Statistics},
	author = {Richardson, Thomas and Spirtes, Peter},
	month = aug,
	year = {2002},
}

@article{gerhardus_high-recall_2021,
	title = {High-recall causal discovery for autocorrelated time series with latent confounders},
	url = {http://arxiv.org/abs/2007.01884},
	abstract = {We present a new method for linear and nonlinear, lagged and contemporaneous constraint-based causal discovery from observational time series in the presence of latent confounders. We show that existing causal discovery methods such as FCI and variants suffer from low recall in the autocorrelated time series case and identify low effect size of conditional independence tests as the main reason. Information-theoretical arguments show that effect size can often be increased if causal parents are included in the conditioning sets. To identify parents early on, we suggest an iterative procedure that utilizes novel orientation rules to determine ancestral relationships already during the edge removal phase. We prove that the method is order-independent, and sound and complete in the oracle case. Extensive simulation studies for different numbers of variables, time lags, sample sizes, and further cases demonstrate that our method indeed achieves much higher recall than existing methods for the case of autocorrelated continuous variables while keeping false positives at the desired level. This performance gain grows with stronger autocorrelation. At https://github.com/jakobrunge/tigramite we provide Python code for all methods involved in the simulation studies.},
	urldate = {2022-01-30},
	journal = {arXiv:2007.01884 [cs, stat]},
	author = {Gerhardus, Andreas and Runge, Jakob},
	month = feb,
	year = {2021},
	note = {arXiv: 2007.01884},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@article{colombo_chr_2014,
	title = {chr: pc-pseudo code. {Order}-{Independent} {Constraint}-{Based} {Causal} {Structure} {Learning}},
	abstract = {We consider constraint-based methods for causal structure learning, such as the PC-, FCI-, RFCI- and CCD- algorithms (Spirtes et al., 1993, 2000; Richardson, 1996; Colombo et al., 2012; Claassen et al., 2013). The ﬁrst step of all these algorithms consists of the adjacency search of the PC-algorithm. The PC-algorithm is known to be order-dependent, in the sense that the output can depend on the order in which the variables are given. This order-dependence is a minor issue in low-dimensional settings. We show, however, that it can be very pronounced in high-dimensional settings, where it can lead to highly variable results. We propose several modiﬁcations of the PC-algorithm (and hence also of the other algorithms) that remove part or all of this order-dependence. All proposed modiﬁcations are consistent in high-dimensional settings under the same conditions as their original counterparts. We compare the PC-, FCI-, and RFCI-algorithms and their modiﬁcations in simulation studies and on a yeast gene expression data set. We show that our modiﬁcations yield similar performance in low-dimensional settings and improved performance in high-dimensional settings. All software is implemented in the R-package pcalg.},
	language = {en},
	author = {Colombo, Diego and Maathuis, Marloes H},
	month = nov,
	year = {2014},
	pages = {42},
}

@article{colombo_order-independent_nodate-1,
	title = {Order-{Independent} {Constraint}-{Based} {Causal} {Structure} {Learning}},
	abstract = {We consider constraint-based methods for causal structure learning, such as the PC-, FCI-, RFCI- and CCD- algorithms (Spirtes et al., 1993, 2000; Richardson, 1996; Colombo et al., 2012; Claassen et al., 2013). The ﬁrst step of all these algorithms consists of the adjacency search of the PC-algorithm. The PC-algorithm is known to be order-dependent, in the sense that the output can depend on the order in which the variables are given. This order-dependence is a minor issue in low-dimensional settings. We show, however, that it can be very pronounced in high-dimensional settings, where it can lead to highly variable results. We propose several modiﬁcations of the PC-algorithm (and hence also of the other algorithms) that remove part or all of this order-dependence. All proposed modiﬁcations are consistent in high-dimensional settings under the same conditions as their original counterparts. We compare the PC-, FCI-, and RFCI-algorithms and their modiﬁcations in simulation studies and on a yeast gene expression data set. We show that our modiﬁcations yield similar performance in low-dimensional settings and improved performance in high-dimensional settings. All software is implemented in the R-package pcalg.},
	language = {en},
	author = {Colombo, Diego and Maathuis, Marloes H},
	pages = {42},
}

@article{gerhardus_lpcmci_2020,
	title = {{LPCMCI} - {High}-recall causal discovery for autocorrelated time series with latent confounders},
	abstract = {We present a new method for linear and nonlinear, lagged and contemporaneous constraint-based causal discovery from observational time series in the presence of latent confounders. We show that existing causal discovery methods such as FCI and variants suffer from low recall in the autocorrelated time series case and identify low effect size of conditional independence tests as the main reason. Information-theoretical arguments show that effect size can often be increased if causal parents are included in the conditioning sets. To identify parents early on, we suggest an iterative procedure that utilizes novel orientation rules to determine ancestral relationships already during the edge removal phase. We prove that the method is order-independent, and sound and complete in the oracle case. Extensive simulation studies for different numbers of variables, time lags, sample sizes, and further cases demonstrate that our method indeed achieves much higher recall than existing methods for the case of autocorrelated continuous variables while keeping false positives at the desired level. This performance gain grows with stronger autocorrelation. At github.com/jakobrunge/tigramite we provide Python code for all methods involved in the simulation studies.},
	language = {en},
	author = {Gerhardus, Andreas and Runge, Jakob},
	month = jul,
	year = {2020},
	pages = {11},
}

@misc{gerhardus_lpcmci_2021,
	title = {{LPCMCI}: {Causal} {Discovery} in {Time} {Series} with {Latent} {Confounders}},
	shorttitle = {{LPCMCI}},
	url = {https://ui.adsabs.harvard.edu/abs/2021EGUGA..23.8259G},
	abstract = {The quest to understand cause and effect relationships is at the basis of the scientific enterprise. In cases where the classical approach of controlled experimentation is not feasible, methods from the modern framework of causal discovery provide an alternative way to learn about cause and effect from observational, i.e., non-experimental data. Recent years have seen an increasing interest in these methods from various scientific fields, for example in the climate and Earth system sciences (where large scale experimentation is often infeasible) as well as machine learning and artificial intelligence (where models based on an understanding of cause and effect promise to be more robust under changing conditions.)In this contribution we present the novel LPCMCI algorithm for learning the cause and effect relationships in multivariate time series. The algorithm is specifically adapted to several challenges that are prevalent in time series considered in the climate and Earth system sciences, for example strong autocorrelations, combinations of time lagged and contemporaneous causal relationships, as well as nonlinearities. It moreover allows for the existence of latent confounders, i.e., it allows for unobserved common causes. While this complication is faced in most realistic scenarios, especially when investigating a system as complex as Earth's climate system, it is nevertheless assumed away in many existing algorithms. We demonstrate applications of LPCMCI to examples from a climate context and compare its performance to competing methods.Related reference:Gerhardus, Andreas and Runge, Jakob (2020). High-recall causal discovery for autocorrelated time series with latent confounders. In Advances in Neural Information Processing Systems 33 pre-proceedings (NeurIPS 2020).},
	urldate = {2021-12-17},
	author = {Gerhardus, Andreas and Runge, Jakob},
	month = apr,
	year = {2021},
	note = {Conference Name: EGU General Assembly Conference Abstracts
ADS Bibcode: 2021EGUGA..23.8259G},
}

@article{runge_pcmci_2020,
	title = {{PCMCI}+ - {Discovering} contemporaneous and lagged causal relations in autocorrelated nonlinear time series datasets},
	abstract = {The paper introduces a novel conditional independence (CI) based method for linear and nonlinear, lagged and contemporaneous causal discovery from observational time series in the causally sufﬁcient case. Existing CI-based methods such as the PC algorithm and also common methods from other frameworks suffer from low recall and partially inﬂated false positives for strong autocorrelation which is an ubiquitous challenge in time series. The novel method, PCMCI+, extends PCMCI [Runge et al., 2019b] to include discovery of contemporaneous links. PCMCI+ improves the reliability of CI tests by optimizing the choice of conditioning sets and even beneﬁts from autocorrelation. The method is order-independent and consistent in the oracle case. A broad range of numerical experiments demonstrates that PCMCI+ has higher adjacency detection power and especially more contemporaneous orientation recall compared to other methods while better controlling false positives. Optimized conditioning sets also lead to much shorter runtimes than the PC algorithm. PCMCI+ can be of considerable use in many real world application scenarios where often time resolutions are too coarse to resolve time delays and strong autocorrelation is present.},
	language = {en},
	author = {Runge, Jakob},
	year = {2020},
	pages = {46},
}

@article{reiser_predicting_2021,
	title = {Predicting and {Visualizing} {Daily} {Mood} of {People} {Using} {Tracking} {Data} of {Consumer} {Devices} and {Services}},
	copyright = {Creative Commons Attribution-ShareAlike 4.0 International License (CC-BY-SA)},
	abstract = {Users can easily export personal data from devices (e.g., weather station and fitness tracker) and services (e.g., screentime tracker and commits on GitHub) they use but struggle to gain valuable insights. To tackle this problem, we present the self-tracking meta app called InsightMe, which aims to show users how data relate to their wellbeing, health, and performance. This paper focuses on mood, which is closely associated with wellbeing. With data collected by one person, we show how a person’s sleep, exercise, nutrition, weather, air quality, screentime, and work correlate to the average mood the person experiences during the day. Furthermore, the app predicts the mood via multiple linear regression and a neural network, achieving an explained variance of 55\% and 50\%, respectively. We strive for explainability and transparency by showing the users p-values of the correlations, drawing prediction intervals. In addition, we conducted a small A/B test on illustrating how the original data influence predictions. The source code1 and app2 are available online.},
	language = {en},
	author = {Reiser, Christian},
	month = dec,
	year = {2021},
	pages = {11},
}

@article{chorin_numerical_1968,
	title = {Numerical solution of the {Navier}-{Stokes} equations},
	volume = {22},
	issn = {0025-5718, 1088-6842},
	url = {https://www.ams.org/mcom/1968-22-104/S0025-5718-1968-0242392-2/},
	doi = {10.1090/S0025-5718-1968-0242392-2},
	abstract = {A finite-difference method for solving the time-dependent NavierStokes equations for an incompressible fluid is introduced. This method uses the primitive variables, i.e. the velocities and the pressure, and is equally applicable to problems in two and three space dimensions. Test problems are solved, and an application to a three-dimensional convection problem is presented.},
	language = {en},
	number = {104},
	urldate = {2021-12-16},
	journal = {Mathematics of Computation},
	author = {Chorin, Alexandre Joel},
	year = {1968},
	pages = {745--762},
}

@inproceedings{runge_conditional_2018,
	title = {Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information},
	url = {https://proceedings.mlr.press/v84/runge18a.html},
	abstract = {Conditional independence testing is a fundamental problem underlying causal discovery and a particularly challenging task in the presence of nonlinear dependencies. Here a fully non-parametric test for continuous data based on conditional mutual information combined with a local permutation scheme is presented. Numerical experiments covering sample sizes from 505050 to 2,0002,0002,000 and dimensions up to 101010 demonstrate that the test reliably generates the null distribution. For smooth nonlinear dependencies, the test has higher power than kernel-based tests in lower dimensions and similar or slightly lower power in higher dimensions. For highly non-smooth densities the data-adaptive nearest neighbor approach is particularly well-suited while kernel methods yield much lower power. The experiments also show that kernel methods utilizing an analytical approximation of the null distribution are not well-calibrated for sample sizes below 1,0001,0001,000. Combining the local permutation scheme with these kernel tests leads to better calibration but lower power.  For smaller sample sizes and lower dimensions, the proposed test is faster than random fourier feature-based kernel tests if (embarrassingly) parallelized, but the runtime increases more sharply with sample size and dimensionality. Thus, more theoretical research to analytically approximate the null distribution and speed up the estimation is desirable.  As illustrated on real data here, the test is ideally suited in combination with causal discovery algorithms.},
	language = {en},
	urldate = {2021-12-14},
	booktitle = {Proceedings of the {Twenty}-{First} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Runge, Jakob},
	month = mar,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {938--947},
}

@article{nauta_causal_2019,
	title = {Causal {Discovery} with {Attention}-{Based} {Convolutional} {Neural} {Networks}},
	volume = {1},
	issn = {2504-4990},
	url = {https://www.mdpi.com/2504-4990/1/1/19},
	doi = {10.3390/make1010019},
	abstract = {Having insight into the causal associations in a complex system facilitates decision making, e.g., for medical treatments, urban infrastructure improvements or ﬁnancial investments. The amount of observational data grows, which enables the discovery of causal relationships between variables from observation of their behaviour in time. Existing methods for causal discovery from time series data do not yet exploit the representational power of deep learning. We therefore present the Temporal Causal Discovery Framework (TCDF), a deep learning framework that learns a causal graph structure by discovering causal relationships in observational time series data. TCDF uses attention-based convolutional neural networks combined with a causal validation step. By interpreting the internal parameters of the convolutional networks, TCDF can also discover the time delay between a cause and the occurrence of its effect. Our framework learns temporal causal graphs, which can include confounders and instantaneous effects. Experiments on ﬁnancial and neuroscientiﬁc benchmarks show state-of-the-art performance of TCDF on discovering causal relationships in continuous time series data. Furthermore, we show that TCDF can circumstantially discover the presence of hidden confounders. Our broadly applicable framework can be used to gain novel insights into the causal dependencies in a complex system, which is important for reliable predictions, knowledge discovery and data-driven decision making.},
	language = {en},
	number = {1},
	urldate = {2021-12-14},
	journal = {Machine Learning and Knowledge Extraction},
	author = {Nauta, Meike and Bucur, Doina and Seifert, Christin},
	month = jan,
	year = {2019},
	pages = {312--340},
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} {Via} the {Lasso}},
	volume = {58},
	issn = {2517-6161},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1996.tb02080.x},
	doi = {10.1111/j.2517-6161.1996.tb02080.x},
	abstract = {We propose a new method for estimation in linear models. The ‘lasso’ minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	language = {en},
	number = {1},
	urldate = {2021-12-13},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1996.tb02080.x},
	keywords = {quadratic programming, regression, shrinkage, subset selection},
	pages = {267--288},
}

@article{runge_causal_2018,
	title = {Causal network reconstruction from time series: {From} theoretical assumptions to practical estimation},
	volume = {28},
	issn = {1054-1500},
	shorttitle = {Causal network reconstruction from time series},
	url = {https://aip.scitation.org/doi/full/10.1063/1.5025050},
	doi = {10.1063/1.5025050},
	abstract = {Causal network reconstruction from time series is an emerging topic in many fields of science. Beyond inferring directionality between two time series, the goal of causal network reconstruction or causal discovery is to distinguish direct from indirect dependencies and common drivers among multiple time series. Here, the problem of inferring causal networks including time lags from multivariate time series is recapitulated from the underlying causal assumptions to practical estimation problems. Each aspect is illustrated with simple examples including unobserved variables, sampling issues, determinism, stationarity, nonlinearity, measurement error, and significance testing. The effects of dynamical noise, autocorrelation, and high dimensionality are highlighted in comparison studies of common causal reconstruction methods. Finally, method performance evaluation approaches and criteria are suggested. The article is intended to briefly review and accessibly illustrate the foundations and practical problems of time series-based causal discovery and stimulate further methodological developments.},
	number = {7},
	urldate = {2021-12-08},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Runge, J.},
	month = jul,
	year = {2018},
	note = {Publisher: American Institute of Physics},
	pages = {075310},
}

@misc{yang_causality_2020,
	title = {Causality: {An} {Introduction}},
	language = {en},
	author = {Yang, Bin},
	month = dec,
	year = {2020},
}

@inproceedings{weichwald_causal_2020,
	title = {Causal structure learning from time series: {Large} regression coefficients may predict causal links better in practice than small p-values},
	shorttitle = {Causal structure learning from time series},
	url = {https://proceedings.mlr.press/v123/weichwald20a.html},
	abstract = {In this article, we describe the algorithms for causal structure learning from time series data that won the Causality 4 Climate competition at the Conference on Neural Information Processing Systems 2019 (NeurIPS). We examine how our combination of established ideas achieves competitive performance on semi-realistic and realistic time series data exhibiting common challenges in real-world Earth sciences data. In particular, we discuss a) a rationale for leveraging linear methods to identify causal links in non-linear systems, b) a simulation-backed explanation as to why large regression coefficients may predict causal links better in practice than small p-values and thus why normalising the data may sometimes hinder causal structure learning. For benchmark usage, we detail the algorithms here and provide implementations at \{https://github.com/sweichwald/tidybench\}. We propose the presented competition-proven methods for baseline benchmark comparisons to guide the development of novel algorithms for structure learning from time series.},
	language = {en},
	urldate = {2021-12-02},
	booktitle = {Proceedings of the {NeurIPS} 2019 {Competition} and {Demonstration} {Track}},
	publisher = {PMLR},
	author = {Weichwald, Sebastian and Jakobsen, Martin E. and Mogensen, Phillip B. and Petersen, Lasse and Thams, Nikolaj and Varando, Gherardo},
	month = aug,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {27--36},
}

@article{sachs_causal_2005,
	title = {Causal {Protein}-{Signaling} {Networks} {Derived} from {Multiparameter} {Single}-{Cell} {Data}},
	volume = {308},
	url = {https://www.science.org/doi/abs/10.1126/science.1105809},
	doi = {10.1126/science.1105809},
	number = {5721},
	urldate = {2021-12-02},
	journal = {Science},
	author = {Sachs, Karen and Perez, Omar and Pe'er, Dana and Lauffenburger, Douglas A. and Nolan, Garry P.},
	month = apr,
	year = {2005},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {523--529},
}

@article{mooij_distinguishing_2016,
	title = {Distinguishing {Cause} from {Effect} {Using} {Observational} {Data}: {Methods} and {Benchmarks}},
	volume = {17},
	issn = {1533-7928},
	shorttitle = {Distinguishing {Cause} from {Effect} {Using} {Observational} {Data}},
	url = {http://jmlr.org/papers/v17/14-518.html},
	abstract = {The discovery of causal relationships from purely observational data is a fundamental problem in science. The most elementary form of such a causal discovery problem is to decide whether 
X
X
 causes 
Y
Y
 or, alternatively, 
Y
Y
 causes 
X
X
, given joint observations of two variables 
X,Y
X,Y
. An example is to decide whether altitude causes temperature, or vice versa, given only joint measurements of both variables. Even under the simplifying assumptions of no confounding, no feedback loops, and no selection bias, such bivariate causal discovery problems are challenging. Nevertheless, several approaches for addressing those problems have been proposed in recent years. We review two families of such methods: methods based on Additive Noise Models (ANMs) and Information Geometric Causal Inference (IGCI). We present the benchmark CauseEffectPairs that consists of data for 100 different cause-effect pairs selected from 37 data sets from various domains (e.g., meteorology, biology, medicine, engineering, economy, etc.) and motivate our decisions regarding the ground truth causal directions of all pairs. We evaluate the performance of several bivariate causal discovery methods on these real-world benchmark data and in addition on artificially simulated data. Our empirical results on real-world data indicate that certain methods are indeed able to distinguish cause from effect using only purely observational data, although more benchmark data would be needed to obtain statistically significant conclusions. One of the best performing methods overall is the method based on Additive Noise Models that has originally been proposed by Hoyer et al. (2009), which obtains an accuracy of 63 
±
±
 10 \% and an AUC of 0.74 
±
±
 0.05 on the real-world benchmark. As the main theoretical contribution of this work we prove the consistency of that method.},
	number = {32},
	urldate = {2021-12-02},
	journal = {Journal of Machine Learning Research},
	author = {Mooij, Joris M. and Peters, Jonas and Janzing, Dominik and Zscheischler, Jakob and Schölkopf, Bernhard},
	year = {2016},
	pages = {1--102},
}

@book{peters_elements_2018,
	title = {Elements of causal inference: foundations and learning algorithms},
	volume = {88},
	shorttitle = {Elements of causal inference},
	url = {https://www.tandfonline.com/doi/full/10.1080/00949655.2018.1505197},
	language = {en},
	urldate = {2021-11-10},
	author = {Peters, Jonas and Janzig, Dominik and Schölkopf, Bernhard},
	month = nov,
	year = {2018},
}

@article{reick_estimation_2021,
	title = {Estimation of {Bivariate} {Structural} {Causal} {Models} by {Variational} {Gaussian} {Process} {Regression} {Under} {Likelihoods} {Parametrised} by {Normalising} {Flows}},
	url = {http://arxiv.org/abs/2109.02521},
	abstract = {One major drawback of state-of-the-art artificial intelligence is its lack of explainability. One approach to solve the problem is taking causality into account. Causal mechanisms can be described by structural causal models. In this work, we propose a method for estimating bivariate structural causal models using a combination of normalising flows applied to density estimation and variational Gaussian process regression for post-nonlinear models. It facilitates causal discovery, i.e. distinguishing cause and effect, by either the independence of cause and residual or a likelihood ratio test. Our method which estimates post-nonlinear models can better explain a variety of real-world cause-effect pairs than a simple additive noise model. Though it remains difficult to exploit this benefit regarding all pairs from the T{\textbackslash}"ubingen benchmark database, we demonstrate that combining the additive noise model approach with our method significantly enhances causal discovery.},
	urldate = {2021-11-10},
	journal = {arXiv:2109.02521 [cs, stat]},
	author = {Reick, Nico and Wiewel, Felix and Bartler, Alexander and Yang, Bin},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.02521},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {10},
}

@misc{noauthor_happimeter_nodate,
	title = {Happimeter - {Apps} on {Google} {Play}},
	url = {https://play.google.com/store/apps/details?id=com.happimeterteam.happimeter&hl=en&gl=US},
	abstract = {Track your mood to identify what causes certain emotional states},
	language = {en},
	urldate = {2021-11-29},
}

@article{runge_inferring_2019,
	title = {Inferring causation from time series in {Earth} system sciences},
	volume = {10},
	copyright = {2019 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-019-10105-3},
	doi = {10.1038/s41467-019-10105-3},
	abstract = {The heart of the scientific enterprise is a rational effort to understand the causes behind the phenomena we observe. In large-scale complex dynamical systems such as the Earth system, real experiments are rarely feasible. However, a rapidly increasing amount of observational and simulated data opens up the use of novel data-driven causal methods beyond the commonly adopted correlation techniques. Here, we give an overview of causal inference frameworks and identify promising generic application cases common in Earth system sciences and beyond. We discuss challenges and initiate the benchmark platform causeme.netto close the gap between method users and developers.},
	language = {en},
	number = {1},
	urldate = {2021-11-25},
	journal = {Nature Communications},
	author = {Runge, Jakob and Bathiany, Sebastian and Bollt, Erik and Camps-Valls, Gustau and Coumou, Dim and Deyle, Ethan and Glymour, Clark and Kretschmer, Marlene and Mahecha, Miguel D. and Muñoz-Marí, Jordi and van Nes, Egbert H. and Peters, Jonas and Quax, Rick and Reichstein, Markus and Scheffer, Marten and Schölkopf, Bernhard and Spirtes, Peter and Sugihara, George and Sun, Jie and Zhang, Kun and Zscheischler, Jakob},
	month = jun,
	year = {2019},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Climate sciences;Computational science;Databases;Environmental sciences;Statistical physics, thermodynamics and nonlinear dynamics
Subject\_term\_id: climate-sciences;computational-science;databases;environmental-sciences;statistical-physics-thermodynamics-and-nonlinear-dynamics},
	keywords = {Climate sciences, Computational science, Databases, Environmental sciences, Statistical physics, thermodynamics and nonlinear dynamics},
	pages = {13},
}

@unpublished{neal_introduction_2020,
	title = {Introduction to {Causal} {Inference}},
	language = {en},
	author = {Neal, Brady},
	month = dec,
	year = {2020},
}

@article{eberhardt_introduction_2017,
	title = {Introduction to the foundations of causal discovery},
	volume = {3},
	issn = {2364-415X, 2364-4168},
	url = {http://link.springer.com/10.1007/s41060-016-0038-6},
	doi = {10.1007/s41060-016-0038-6},
	abstract = {This article presents an overview of several known approaches to causal discovery. It is organized by relating the different fundamental assumptions that the methods depend on. The goal is to indicate that for a large variety of different settings the assumptions necessary and sufﬁcient for causal discovery are now well understood.},
	language = {en},
	number = {2},
	urldate = {2021-11-25},
	journal = {International Journal of Data Science and Analytics},
	author = {Eberhardt, Frederick},
	month = mar,
	year = {2017},
	pages = {81--91},
}

@article{glymour_review_2019,
	title = {Review of {Causal} {Discovery} {Methods} {Based} on {Graphical} {Models}},
	volume = {10},
	issn = {1664-8021},
	url = {https://www.frontiersin.org/article/10.3389/fgene.2019.00524},
	doi = {10.3389/fgene.2019.00524},
	abstract = {A fundamental task in various disciplines of science, including biology, is to find underlying causal relations and make use of them. Causal relations can be seen if interventions are properly applied; however, in many cases they are difficult or even impossible to conduct. It is then necessary to discover causal relations by analyzing statistical properties of purely observational data, which is known as causal discovery or causal structure search. This paper aims to give a introduction to and a brief review of the computational methods for causal discovery that were developed in the past three decades, including constraint-based and score-based methods and those based on functional causal models, supplemented by some illustrations and applications.},
	urldate = {2021-11-25},
	journal = {Frontiers in Genetics},
	author = {Glymour, Clark and Zhang, Kun and Spirtes, Peter},
	year = {2019},
	pages = {524},
}

@article{shimizu_linear_2006,
	title = {A {Linear} {Non}-{Gaussian} {Acyclic} {Model} for {Causal} {Discovery}},
	abstract = {In recent years, several methods have been proposed for the discovery of causal structure from non-experimental data. Such methods make various assumptions on the data generating process to facilitate its identiﬁcation from purely observational data. Continuing this line of research, we show how to discover the complete causal structure of continuous-valued data, under the assumptions that (a) the data generating process is linear, (b) there are no unobserved confounders, and (c) disturbance variables have non-Gaussian distributions of non-zero variances. The solution relies on the use of the statistical method known as independent component analysis, and does not require any pre-speciﬁed time-ordering of the variables. We provide a complete Matlab package for performing this LiNGAM analysis (short for Linear Non-Gaussian Acyclic Model), and demonstrate the effectiveness of the method using artiﬁcially generated data and real-world data.},
	language = {en},
	author = {Shimizu, Shohei and Hoyer, Patrik O and Hyvarinen, Aapo and Kerminen, Antti},
	year = {2006},
	pages = {28},
}

@article{kalainathan_structural_2018,
	title = {Structural {Agnostic} {Modeling}: {Adversarial} {Learning} of {Causal} {Graphs}},
	shorttitle = {Structural {Agnostic} {Modeling}},
	url = {http://arxiv.org/abs/1803.04929},
	abstract = {A new causal discovery method, Structural Agnostic Modeling (SAM), is presented in this paper. Leveraging both conditional independencies and distributional asymmetries in the data, SAM aims to ﬁnd the underlying causal structure from observational data. The approach is based on a game between different players estimating each variable distribution conditionally to the others as a neural net, and an adversary aimed at discriminating the overall joint conditional distribution, and that of the original data. A learning criterion combining distribution estimation, sparsity and acyclicity constraints is used to enforce the end-to-end optimization of the graph structure and parameters through stochastic gradient descent. Besides a theoretical analysis of the approach in the large sample limit, SAM is extensively experimentally validated on synthetic and real data.},
	language = {en},
	urldate = {2021-11-10},
	journal = {arXiv:1803.04929 [stat]},
	author = {Kalainathan, Diviyan and Goudet, Olivier and Guyon, Isabelle and Lopez-Paz, David and Sebag, Michèle},
	year = {2018},
	note = {arXiv: 1803.04929},
	keywords = {Statistics - Machine Learning},
}

@book{pearl_book_2018,
	address = {New York},
	title = {The book of why: the new science of cause and effect},
	isbn = {978-0-465-09761-6},
	shorttitle = {The book of why},
	abstract = {"Everyone has heard the claim, "Correlation does not imply causation." What might sound like a reasonable dictum metastasized in the twentieth century into one of science's biggest obstacles, as a legion of researchers became unwilling to make the claim that one thing could cause another. Even two decades ago, asking a statistician a question like "Was it the aspirin that stopped my headache?" would have been like asking if he believed in voodoo, or at best a topic for conversation at a cocktail party rather than a legitimate target of scientific inquiry. Scientists were allowed to posit only that the probability that one thing was associated with another. This all changed with Judea Pearl, whose work on causality was not just a victory for common sense, but a revolution in the study of the world"--},
	language = {en},
	publisher = {Basic Books},
	author = {Pearl, Judea and Mackenzie, Dana},
	year = {2018},
	keywords = {Causation, Inference},
}

@article{aliferis_time_nodate,
	title = {Time and {Sample} {Efficient} {Discovery} of {Markov} {Blankets} and {Direct} {Causal} {Relations}},
	abstract = {Data Mining with Bayesian Network learning has two important characteristics: under broad conditions learned edges between variables correspond to causal inﬂuences, and second, for every variable T in the network a special subset (Markov Blanket) identiﬁable by the network is the minimal variable set required to predict T . However, all known algorithms learning a complete BN do not scale up beyond a few hundred variables. On the other hand, all known sound algorithms learning a local region of the network require an exponential number of training instances to the size of the learned region.},
	language = {en},
	author = {Aliferis, Constantin and Statnikov, Alexander},
	pages = {6},
}

@book{spirtes_causation_1993,
	address = {New York, NY},
	series = {Lecture {Notes} in {Statistics}},
	title = {Causation, {Prediction}, and {Search}},
	volume = {81},
	isbn = {978-1-4612-7650-0 978-1-4612-2748-9},
	url = {http://link.springer.com/10.1007/978-1-4612-2748-9},
	language = {en},
	urldate = {2021-11-23},
	publisher = {Springer New York},
	author = {Spirtes, Peter and Glymour, Clark and Scheines, Richard},
	editor = {Berger, J. and Fienberg, S. and Gani, J. and Krickeberg, K. and Olkin, I. and Singer, B.},
	year = {1993},
	doi = {10.1007/978-1-4612-2748-9},
}

@unpublished{sontag_lecture_nodate,
	title = {Lecture 14: {Causal} {Inference}, {Part} {I}},
	language = {en},
	author = {Sontag, David and Szolovits, Peter and Guryev, Georgy},
}

@unpublished{sontag_lecture_nodate-1,
	title = {Lecture 15: {Causal} {Inference} {Part} {II}},
	language = {en},
	author = {Sontag, David and Szolovits, Peter},
}

@book{pearl_causality_2000,
	address = {Cambridge, U.K. ; New York},
	title = {Causality: models, reasoning, and inference},
	isbn = {978-0-521-89560-6 978-0-521-77362-1},
	shorttitle = {Causality},
	publisher = {Cambridge University Press},
	author = {Pearl, Judea},
	year = {2000},
	keywords = {Causation, Probabilities},
}

@book{mcelreath_statistical_2018,
	edition = {1},
	title = {Statistical {Rethinking}: {A} {Bayesian} {Course} with {Examples} in {R} and {Stan}},
	isbn = {978-1-315-37249-5},
	shorttitle = {Statistical {Rethinking}},
	url = {https://www.taylorfrancis.com/books/9781315362618},
	language = {en},
	urldate = {2021-11-22},
	publisher = {Chapman and Hall/CRC},
	author = {McElreath, Richard},
	month = jan,
	year = {2018},
	doi = {10.1201/9781315372495},
}

@article{pearl_causal_2016,
	title = {Causal {Inference} in {Statistics}},
	language = {en},
	author = {Pearl, Judea},
	year = {2016},
	pages = {159},
}

@article{reick_estimation_2020,
	title = {Estimation of {Structural} {Causal} {Models} by {Variational} {Gaussian} {Process} {Regression}},
	abstract = {One major drawback of state-of-the-art artiﬁcial intelligence is its lack of explainability. One approach to solve the problem is taking causality into account instead of only correlations. Causal mechanisms can be described by structural causal models. In this work, we propose a method to estimate bivariate structural causal models by combining normalizing ﬂows, which can be applied in density estimation, and variational Gaussian process regression. Though the method proves useful with artiﬁcial datasets, identifying cause and eﬀect on practical data is still an unresolved problem.},
	language = {en},
	author = {Reick, Nico},
	month = nov,
	year = {2020},
	pages = {73},
}

@article{benjamini_controlling_1995,
	title = {Controlling the {False} {Discovery} {Rate}: {A} {Practical} and {Powerful} {Approach} to {Multiple} {Testing}},
	volume = {57},
	issn = {2517-6161},
	shorttitle = {Controlling the {False} {Discovery} {Rate}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1995.tb02031.x},
	doi = {10.1111/j.2517-6161.1995.tb02031.x},
	abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses — the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferronitype procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
	language = {en},
	number = {1},
	urldate = {2021-11-19},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Benjamini, Yoav and Hochberg, Yosef},
	year = {1995},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1995.tb02031.x},
	keywords = {bonferroni-type procedures, familywise error rate, multiple-comparison procedures, p-values},
	pages = {289--300},
}

@article{vosburg_effects_1998,
	title = {The {Effects} of {Positive} and {Negative} {Mood} on {Divergent}-{Thinking} {Performance}},
	volume = {11},
	issn = {1040-0419},
	url = {https://doi.org/10.1207/s15326934crj1102_6},
	doi = {10.1207/s15326934crj1102_6},
	abstract = {Previous work has shown that positive mood may facilitate creative problem solving. However, studies have also shown positive mood may be detrimental to creative thinking under conditions favoring an optimizing strategy for solution. It is argued herein that the opposite effect is observed under conditions promoting loose processing and satisficing problem-solving strategies. The effects of positive and negative mood on divergent-thinking performance were examined in a quasi-experimental design. The sample comprised 188 arts and psychology students. Mood was measured with an adjective checklist prior to task performance. Real-life divergent-thinking tasks scored for fluency were used as the dependent variables. Results showed natural positive mood to facilitate significantly task performance and negative mood to inhibit it. The re was no effect of arousal. The results suggest that per sons in elevated moods may prefer satisficing strategies, which would lead to a higher number of proposed solutions. Persons in a negative mood may choose optimizing strategies and be more concerned with the quality of their ideas, which is detrimental to performance on this kind of task.},
	number = {2},
	urldate = {2021-11-19},
	journal = {Creativity Research Journal},
	author = {Vosburg, Suzanne K.},
	month = apr,
	year = {1998},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1207/s15326934crj1102\_6},
	pages = {165--172},
}

@article{brand_how_2007,
	title = {How do we learn in a negative mood? {Effects} of a negative mood on transfer and learning},
	volume = {17},
	issn = {0959-4752},
	shorttitle = {How do we learn in a negative mood?},
	url = {https://www.sciencedirect.com/science/article/pii/S0959475206001150},
	doi = {10.1016/j.learninstruc.2006.11.002},
	abstract = {Findings show that both positive and negative mood may hinder or promote information processing. In two experiments, we show that negative mood impairs transfer effects and learning. In the first experiment, N=54 participants drawn from a training course for the Swiss Corps of Fortification Guards first learned to solve the three- and four-disk Tower of Hanoi (ToH) problem to mastery level. After mood induction, they were asked to solve one proximal (five-disk ToH) and two distal transfer tasks (the Missionary and Cannibal Problem and the Katona Card Problem). Participants in a negative mood solved the transfer tasks less efficiently. In the second experiment, this result was replicated with a sample of N=80 participants drawn from a training course for nurses. Additionally, mood affected performance if it was induced before the learning phase; participants in a negative mood needed more repetitions to reach the mastery level and also performed worse in the transfer tasks, although there were no greater mood differences in this problem-solving phase. The implications for the design of learning settings are discussed.},
	language = {en},
	number = {1},
	urldate = {2021-11-19},
	journal = {Learning and Instruction},
	author = {Brand, Serge and Reimer, Torsten and Opwis, Klaus},
	month = feb,
	year = {2007},
	keywords = {Mastery level, Negative mood, Problem solving, Tower of Hanoi, Transfer},
	pages = {1--16},
}

@misc{binz_kevin_intro_2019,
	title = {Intro to {Regularization}},
	url = {https://kevinbinz.com/2019/06/09/regularization/},
	abstract = {Part Of: Machine Learning sequenceFollowup To: Bias vs Variance, Gradient Descent Content Summary: 1100 words, 11 min read In Intro to Gradient Descent, we discussed how loss functions allow optimi…},
	language = {en},
	urldate = {2021-11-18},
	journal = {Fewer Lacunae},
	author = {{Binz, Kevin}},
	month = jun,
	year = {2019},
}

@misc{cushing_barry_e_mitigation_1996,
	title = {Mitigation of recency bias in audit judgment: {The} effect of documentation - {ProQuest}},
	shorttitle = {Mitigation of recency bias in audit judgment},
	url = {https://www.proquest.com/openview/283dcd106582467195de2fee655220f2/1?pq-origsite=gscholar&cbl=31718},
	abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
	language = {en},
	urldate = {2021-11-18},
	author = {Cushing, Barry E},
	year = {1996},
}

@incollection{skowronski_chapter_2014,
	title = {Chapter {Three} - {The} {Fading} {Affect} {Bias}: {Its} {History}, {Its} {Implications}, and {Its} {Future}},
	volume = {49},
	shorttitle = {Chapter {Three} - {The} {Fading} {Affect} {Bias}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128000526000032},
	abstract = {Recalling a memory often prompts an emotional response. Research examining the fading affect bias (FAB) indicates that the emotional response prompted by positive memories often tends to be stronger than the emotional response prompted by negative memories. This chapter presents an overview of research that has explored the FAB effect. This overview indicates that the FAB reflects two trends: (1) over time, the affect associated with positive memories tends to fade more slowly from event occurrence to event recall than the affect associated with negative memories, and (2) it is more often the case that events that were negative at their occurrence will ultimately come to prompt positive affect-at-recall than it is the case that events that were positive at their occurrence will come to prompt negative affect-at-recall. Research has also revealed that the FAB can be altered by event moderators, situational moderators, and individual-difference moderators. The chapter uses this research review to highlight several needed directions for future research, suggest some theoretical ideas that may underlie the FAB, and discuss some ways in which the FAB might be important to life in a social world.},
	language = {en},
	urldate = {2021-11-18},
	booktitle = {Advances in {Experimental} {Social} {Psychology}},
	publisher = {Academic Press},
	author = {Skowronski, John J. and Walker, W. Richard and Henderson, Dawn X. and Bond, Gary D.},
	editor = {Olson, James M. and Zanna, Mark P.},
	month = jan,
	year = {2014},
	doi = {10.1016/B978-0-12-800052-6.00003-2},
	keywords = {Autobiographical memory, Emotion, Fading affect bias, Memory, Positivity, Self},
	pages = {163--218},
}

@article{samek_explainable_2017,
	title = {Explainable {Artificial} {Intelligence}: {Understanding}, {Visualizing} and {Interpreting} {Deep} {Learning} {Models}},
	shorttitle = {Explainable {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/1708.08296},
	abstract = {With the availability of large databases and recent improvements in deep learning methodology, the performance of AI systems is reaching or even exceeding the human level on an increasing number of complex tasks. Impressive examples of this development can be found in domains such as image classification, sentiment analysis, speech understanding or strategic game playing. However, because of their nested non-linear structure, these highly successful machine learning and artificial intelligence models are usually applied in a black box manner, i.e., no information is provided about what exactly makes them arrive at their predictions. Since this lack of transparency can be a major drawback, e.g., in medical applications, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This paper summarizes recent developments in this field and makes a plea for more interpretability in artificial intelligence. Furthermore, it presents two approaches to explaining predictions of deep learning models, one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables. These methods are evaluated on three classification tasks.},
	urldate = {2021-11-18},
	journal = {arXiv:1708.08296 [cs, stat]},
	author = {Samek, Wojciech and Wiegand, Thomas and Müller, Klaus-Robert},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.08296},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {http://arxiv.org/abs/1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
	urldate = {2021-11-17},
	journal = {arXiv:1711.05101 [cs, math]},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = jan,
	year = {2019},
	note = {arXiv: 1711.05101},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
}

@article{prechelt_automatic_1998,
	title = {Automatic early stopping using cross validation: quantifying the criteria},
	volume = {11},
	issn = {0893-6080},
	shorttitle = {Automatic early stopping using cross validation},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608098000100},
	doi = {10.1016/S0893-6080(98)00010-0},
	abstract = {Cross validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting (`early stopping'). The exact criterion used for cross validation based early stopping, however, is chosen in an ad-hoc fashion by most researchers or training is stopped interactively. To aid a more well-founded selection of the stopping criterion, 14 different automatic stopping criteria from three classes were evaluated empirically for their efficiency and effectiveness in 12 different classification and approximation tasks using multi-layer perceptrons with RPROP training. The experiments show that, on average, slower stopping criteria allow for small improvements in generalization (in the order of 4\%), but cost about a factor of 4 longer in training time.},
	language = {en},
	number = {4},
	urldate = {2021-11-17},
	journal = {Neural Networks},
	author = {Prechelt, Lutz},
	month = jun,
	year = {1998},
	keywords = {Cross validation, Early stopping, Empirical study, Generalization, Overfitting, Supervised learning},
	pages = {761--767},
}

@article{xu_empirical_2015,
	title = {Empirical {Evaluation} of {Rectified} {Activations} in {Convolutional} {Network}},
	url = {http://arxiv.org/abs/1505.00853},
	abstract = {In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart. By using RReLU, we achieved 75.68{\textbackslash}\% accuracy on CIFAR-100 test set without multiple test or ensemble.},
	urldate = {2021-11-17},
	journal = {arXiv:1505.00853 [cs, stat]},
	author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
	month = nov,
	year = {2015},
	note = {arXiv: 1505.00853},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zou_regularization_2005,
	title = {Regularization and variable selection via the elastic net},
	volume = {67},
	issn = {1369-7412, 1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2005.00503.x},
	doi = {10.1111/j.1467-9868.2005.00503.x},
	abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping eﬀect, where strongly correlated predictors tend to be in (out) the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p n case. An eﬃcient algorithm called LARS-EN is proposed for computing elastic net regularization paths eﬃciently, much like the LARS algorithm does for the lasso.},
	language = {en},
	number = {2},
	urldate = {2021-11-17},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Zou, Hui and Hastie, Trevor},
	month = apr,
	year = {2005},
	pages = {301--320},
}

@article{durre_robust_2015,
	title = {Robust estimation of (partial) autocorrelation},
	volume = {7},
	issn = {1939-0068},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.1351},
	doi = {10.1002/wics.1351},
	abstract = {The autocorrelation function (acf) and the partial autocorrelation function (pacf) are elementary tools of linear time series analysis. The sensitivity of the conventional sample acf and pacf to outliers is well known. We review robust estimators and evaluate their performances in different data situations considering Gaussian scenarios with and without outliers as well as times series with heavy tails in a simulation study. WIREs Comput Stat 2015, 7:205–222. doi: 10.1002/wics.1351 This article is categorized under: Statistical and Graphical Methods of Data Analysis {\textgreater} Robust Methods Data: Types and Structure {\textgreater} Time Series, Stochastic Processes, and Functional Data},
	language = {en},
	number = {3},
	urldate = {2021-11-17},
	journal = {WIREs Computational Statistics},
	author = {Dürre, Alexander and Fried, Roland and Liboschik, Tobias},
	year = {2015},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1351},
	keywords = {autocovariance, correlogram, outliers, time series},
	pages = {205--222},
}

@misc{noauthor_bipolar_nodate,
	title = {Bipolar disorder - {Symptoms} and causes},
	url = {https://www.mayoclinic.org/diseases-conditions/bipolar-disorder/symptoms-causes/syc-20355955},
	abstract = {Bipolar disorder causes extreme mood swings that include emotional highs (mania or hypomania) and lows (depression).},
	language = {en},
	urldate = {2021-11-16},
	journal = {Mayo Clinic},
}

@book{symposium_neurobiology_1984,
	title = {The {Neurobiology} of {Pain}: {Symposium} of the {Northern} {Neurobiology} {Group}, {Held} at {Leeds} on 18 {April}, 1983},
	isbn = {978-0-7190-1061-3},
	shorttitle = {The {Neurobiology} of {Pain}},
	language = {en},
	publisher = {Manchester University Press},
	author = {Symposium, Northern Neurobiology Group (Great Britain) and Group, Northern Neurobiology},
	year = {1984},
	note = {Google-Books-ID: S7rnAAAAIAAJ},
	keywords = {Medical / General},
}

@article{fedorikhin_positive_2010,
	title = {Positive {Mood} and {Resistance} to {Temptation}: {The} {Interfering} {Influence} of {Elevated} {Arousal}},
	volume = {37},
	issn = {0093-5301},
	shorttitle = {Positive {Mood} and {Resistance} to {Temptation}},
	url = {https://doi.org/10.1086/655665},
	doi = {10.1086/655665},
	abstract = {We investigate the interfering influence of elevated arousal on the impact of positive mood on resistance to temptation. Three studies demonstrate that when a temptation activates long-term health goals, baseline positive mood facilitates resistance to temptation in (1) the choice between two snack items, one of which is more unhealthy, sinful, and hard to resist (M\&amp;Ms) than the other (grapes) and (2) the monitoring of consumption when the sinful option is chosen. However, this influence is attenuated when positive mood is accompanied by elevated arousal. We demonstrate that the cognitive depletion that accompanies elevated arousal interferes with the self-regulatory focus of positive mood, decreasing resistance to temptation.},
	number = {4},
	urldate = {2021-11-16},
	journal = {Journal of Consumer Research},
	author = {Fedorikhin, Alexander and Patrick, Vanessa M.},
	month = dec,
	year = {2010},
	pages = {698--711},
}

@article{xu_l12_2012,
	title = {L1/2 {Regularization}: {A} {Thresholding} {Representation} {Theory} and a {Fast} {Solver}},
	volume = {23},
	issn = {2162-2388},
	shorttitle = {$_{\textrm{1/2}}$ {Regularization}},
	doi = {10.1109/TNNLS.2012.2197412},
	abstract = {The special importance of L1/2 regularization has been recognized in recent studies on sparse modeling (particularly on compressed sensing). The L1/2 regularization, however, leads to a nonconvex, nonsmooth, and non-Lipschitz optimization problem that is difficult to solve fast and efficiently. In this paper, through developing a threshoding representation theory for L1/2 regularization, we propose an iterative half thresholding algorithm for fast solution of L1/2 regularization, corresponding to the well-known iterative soft thresholding algorithm for L1 regularization, and the iterative hard thresholding algorithm for L0 regularization. We prove the existence of the resolvent of gradient of {\textbar}{\textbar}x{\textbar}{\textbar}1/21/2, calculate its analytic expression, and establish an alternative feature theorem on solutions of L1/2 regularization, based on which a thresholding representation of solutions of L1/2 regularization is derived and an optimal regularization parameter setting rule is formulated. The developed theory provides a successful practice of extension of the well- known Moreau's proximity forward-backward splitting theory to the L1/2 regularization case. We verify the convergence of the iterative half thresholding algorithm and provide a series of experiments to assess performance of the algorithm. The experiments show that the half algorithm is effective, efficient, and can be accepted as a fast solver for L1/2 regularization. With the new algorithm, we conduct a phase diagram study to further demonstrate the superiority of L1/2 regularization over L1 regularization.},
	number = {7},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Xu, Zongben and Chang, Xiangyu and Xu, Fengmin and Zhang, Hai},
	month = jul,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {$_{\textrm{q}}$ regularization, Compressive sensing, half, hard, soft, sparsity, thresholding algorithms, thresholding representation theory},
	pages = {1013--1027},
}

@article{hariton_randomised_2018,
	title = {Randomised controlled trials—the gold standard for effectiveness research},
	volume = {125},
	issn = {1470-0328},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6235704/},
	doi = {10.1111/1471-0528.15199},
	number = {13},
	urldate = {2021-11-14},
	journal = {BJOG : an international journal of obstetrics and gynaecology},
	author = {Hariton, Eduardo and Locascio, Joseph J.},
	month = dec,
	year = {2018},
	pmid = {29916205},
	pmcid = {PMC6235704},
	pages = {1716},
}

@article{carlsson_assessment_1983,
	title = {Assessment of chronic pain. {I}. {Aspects} of the reliability and validity of the visual analogue scale:},
	volume = {16},
	issn = {0304-3959},
	shorttitle = {Assessment of chronic pain. {I}. {Aspects} of the reliability and validity of the visual analogue scale},
	url = {http://journals.lww.com/00006396-198305000-00008},
	doi = {10.1016/0304-3959(83)90088-X},
	abstract = {The visual analogue scale (VAS) is a simple and frequently used method for the assessment of variations in intensity of pain. In clinical practice the percentage of pain relief, assessed by VAS, is often considered as a measure of the efficacy of treatment. However, as illustrated in the present study, the validity of VAS estimates performed by patients with chronic pain may be unsatisfactory. Two types of VAS, an absolute and a comparative scale, were compared with respect to factors influencing the reliability and validity of pain estimates. As shown in this study the absolute type of VAS seems to be less sensitive to bias than the comparative one and is therefore preferable for general clinical use. Moreover, the patients appear to differ considerably in their ability to use the VAS reliably. When assessing efficacy of treatment attention should therefore be paid to several complementary indices of pain relief as well as to the individual’s tendency to bias his estimates.},
	language = {en},
	number = {1},
	urldate = {2021-11-14},
	journal = {Pain},
	author = {Carlsson, Anna Maria},
	month = may,
	year = {1983},
	pages = {87--101},
}

@misc{noauthor_pii_nodate,
	title = {{PII}: 0304-3959(83)90088-{X} {\textbar} {Elsevier} {Enhanced} {Reader}},
	shorttitle = {{PII}},
	url = {https://reader.elsevier.com/reader/sd/pii/030439598390088X?token=8EFCEF9E4D772210CB9F4DB19FBBCBBAC0EC681C8C9244DDE8A977D8AD09C8B7111F257A9EA53FD46E19D69C142DC693&originRegion=eu-west-1&originCreation=20211114104038},
	language = {en},
	urldate = {2021-11-14},
	doi = {10.1016/0304-3959(83)90088-X},
	note = {ISSN: 0304-3959},
}

@article{shanmugam_elements_2018,
	title = {Elements of causal inference: foundations and learning algorithms},
	volume = {88},
	issn = {0094-9655, 1563-5163},
	shorttitle = {Elements of causal inference},
	url = {https://www.tandfonline.com/doi/full/10.1080/00949655.2018.1505197},
	doi = {10.1080/00949655.2018.1505197},
	language = {en},
	number = {16},
	urldate = {2021-11-10},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Shanmugam, Ramalingam},
	month = nov,
	year = {2018},
	pages = {3248--3248},
}

@article{castro_causality_2020,
	title = {Causality matters in medical imaging},
	volume = {11},
	issn = {2041-1723},
	url = {http://www.nature.com/articles/s41467-020-17478-w},
	doi = {10.1038/s41467-020-17478-w},
	abstract = {Abstract
            Causal reasoning can shed new light on the major challenges in machine learning for medical imaging: scarcity of high-quality annotated data and mismatch between the development dataset and the target environment. A causal perspective on these issues allows decisions about data collection, annotation, preprocessing, and learning strategies to be made and scrutinized more transparently, while providing a detailed categorisation of potential biases and mitigation techniques. Along with worked clinical examples, we highlight the importance of establishing the causal relationship between images and their annotations, and offer step-by-step recommendations for future studies.},
	language = {en},
	number = {1},
	urldate = {2021-11-10},
	journal = {Nature Communications},
	author = {Castro, Daniel C. and Walker, Ian and Glocker, Ben},
	month = dec,
	year = {2020},
	pages = {3673},
}

@article{castro_joint_nodate,
	title = {Joint {Probabilistic} {Modelling} of {Images} and {Non}-imaging {Covariates}: {A} {Causal} {Perspective}},
	language = {en},
	author = {Castro, Daniel COELHO DE},
	pages = {189},
}

@article{pawlowski_deep_2020,
	title = {Deep {Structural} {Causal} {Models} for {Tractable} {Counterfactual} {Inference}},
	url = {http://arxiv.org/abs/2006.06485},
	abstract = {We formulate a general framework for building structural causal models (SCMs) with deep learning components. The proposed approach employs normalising ﬂows and variational inference to enable tractable inference of exogenous noise variables—a crucial step for counterfactual inference that is missing from existing deep causal learning methods. Our framework is validated on a synthetic dataset built on MNIST as well as on a real-world medical dataset of brain MRI scans. Our experimental results indicate that we can successfully train deep SCMs that are capable of all three levels of Pearl’s ladder of causation: association, intervention, and counterfactuals, giving rise to a powerful new approach for answering causal questions in imaging applications and beyond. The code for all our experiments is available at https://github.com/biomedia-mira/deepscm.},
	language = {en},
	urldate = {2021-11-10},
	journal = {arXiv:2006.06485 [cs, stat]},
	author = {Pawlowski, Nick and Castro, Daniel C. and Glocker, Ben},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.06485},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{pearl_seven_2019,
	title = {The seven tools of causal inference, with reflections on machine learning},
	volume = {62},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3241036},
	doi = {10.1145/3241036},
	abstract = {The kind of causal inference seen in natural human thought can be "algorithmitized" to help produce human-level machine intelligence.},
	language = {en},
	number = {3},
	urldate = {2021-11-10},
	journal = {Communications of the ACM},
	author = {Pearl, Judea},
	month = feb,
	year = {2019},
	pages = {54--60},
}

@book{escalante_explainable_2018,
	address = {Cham},
	series = {The {Springer} {Series} on {Challenges} in {Machine} {Learning}},
	title = {Explainable and {Interpretable} {Models} in {Computer} {Vision} and {Machine} {Learning}},
	isbn = {978-3-319-98130-7 978-3-319-98131-4},
	url = {http://link.springer.com/10.1007/978-3-319-98131-4},
	language = {en},
	urldate = {2021-11-10},
	publisher = {Springer International Publishing},
	editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Baró, Xavier and Güçlütürk, Yağmur and Güçlü, Umut and van Gerven, Marcel},
	year = {2018},
	doi = {10.1007/978-3-319-98131-4},
}

@article{goudet_learning_2018,
	title = {Learning {Functional} {Causal} {Models} with {Generative} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1709.05321},
	doi = {10.1007/978-3-319-98131-4},
	abstract = {We introduce a new approach to functional causal modeling from observational data, called Causal Generative Neural Networks (CGNN). CGNN leverages the power of neural networks to learn a generative model of the joint distribution of the observed variables, by minimizing the Maximum Mean Discrepancy between generated and observed data. An approximate learning criterion is proposed to scale the computational cost of the approach to linear complexity in the number of observations. The performance of CGNN is studied throughout three experiments. Firstly, CGNN is applied to cause-effect inference, where the task is to identify the best causal hypothesis out of “X → Y ” and “Y → X”. Secondly, CGNN is applied to the problem of identifying v-structures and conditional independences. Thirdly, CGNN is applied to multivariate functional causal modeling: given a skeleton describing the direct dependences in a set of random variables X = [X1, . . . , Xd], CGNN orients the edges in the skeleton to uncover the directed acyclic causal graph describing the causal structure of the random variables. On all three tasks, CGNN is extensively assessed on both artiﬁcial and real-world data, comparing favorably to the state-of-the-art. Finally, CGNN is extended to handle the case of confounders, where latent variables are involved in the overall causal model.},
	language = {en},
	urldate = {2021-11-10},
	journal = {arXiv:1709.05321 [stat]},
	author = {Goudet, Olivier and Kalainathan, Diviyan and Caillou, Philippe and Guyon, Isabelle and Lopez-Paz, David and Sebag, Michèle},
	year = {2018},
	note = {arXiv: 1709.05321},
	keywords = {Statistics - Machine Learning},
}

@misc{welltory_welltory_nodate,
	title = {Welltory - guide to a life of health and productivity},
	url = {https://welltory.com/},
	abstract = {Helping millions reach peak health and productivity. A guide that navigates you to the summit of your potential.},
	language = {en-US},
	urldate = {2021-11-08},
	journal = {Welltory},
	author = {Welltory},
}

@misc{elitehrv_best_nodate,
	title = {Best {Heart} {Rate} {Variability} {Monitor} \& {App}},
	url = {https://elitehrv.com/},
	abstract = {Heart Rate Variability (HRV) tracking app for deep insight into your health, stress, recovery and nervous system balance. Elite HRV's heart rate variability insights help you stay in tune with your body and on track toward your goals.},
	language = {en-US},
	urldate = {2021-11-09},
	journal = {Elite HRV},
	author = {EliteHRV},
}

@misc{fitbit_stress_nodate,
	title = {Stress {Management} - {Stress} {Watch} \& {Monitoring} {\textbar} {Fitbit}},
	url = {https://www.fitbit.com/global/us/technology/stress},
	abstract = {Use the Fitbit Sense smartwatch or the Fitbit Charge 5 tracker to understand how your body is handling stress, gain awareness of your emotional well-being and learn strategies to manage your stress better.},
	language = {en},
	urldate = {2021-11-08},
	author = {FitBit},
}

@misc{hellocodeinc_exist_nodate,
	title = {Exist},
	url = {https://exist.io},
	abstract = {By combining data from services you already use, we can help you understand what makes you more happy, productive, and active.},
	urldate = {2021-11-08},
	author = {HelloCodeInc.},
}

@misc{appleinc_ios_nodate,
	title = {{iOS} - {Health}},
	url = {https://www.apple.com/ios/health/},
	abstract = {The Health app on iPhone puts your important health information at your fingertips, including your health records, labs, activity, sleep, and more.},
	language = {en-US},
	urldate = {2021-11-08},
	journal = {Apple},
	author = {AppleInc.},
}

@article{choudhury_predicting_2013,
	title = {Predicting {Depression} via {Social} {Media}},
	abstract = {Major depression constitutes a serious challenge in personal and public health. Tens of millions of people each year suffer from depression and only a fraction receives adequate treatment. We explore the potential to use social media to detect and diagnose major depressive disorder in individuals. We first employ crowdsourcing to compile a set of Twitter users who report being diagnosed with clinical depression, based on a standard psychometric instrument. Through their social media postings over a year preceding the onset of depression, we measure behavioral attributes relating to social engagement, emotion, language and linguistic styles, ego network, and mentions of antidepressant medications. We leverage these behavioral cues, to build a statistical classifier that provides estimates of the risk of depression, before the reported onset. We find that social media contains useful signals for characterizing the onset of depression in individuals, as measured through decrease in social activity, raised negative affect, highly clustered egonetworks, heightened relational and medicinal concerns, and greater expression of religious involvement. We believe our findings and methods may be useful in developing tools for identifying the onset of major depression, for use by healthcare agencies; or on behalf of individuals, enabling those suffering from depression to be more proactive about their mental health.},
	language = {en},
	author = {Choudhury, Munmun De and Gamon, Michael and Counts, Scott and Horvitz, Eric},
	month = jul,
	year = {2013},
	pages = {10},
}

@article{jaques_multi-task_2015,
	title = {Multi-task, {Multi}-{Kernel} {Learning} for {Estimating} {Individual} {Wellbeing}},
	abstract = {We apply a recently proposed technique – Multi-task Multi-Kernel Learning (MTMKL) – to the problem of modeling students’ wellbeing. Because wellbeing is a complex internal state consisting of several related dimensions, Multi-task learning can be used to classify them simultaneously. Multiple Kernel Learning is used to efﬁciently combine data from multiple modalities. MTMKL combines these approaches using an optimization function similar to a support vector machine (SVM). We show that MTMKL successfully classiﬁes ﬁve dimensions of wellbeing, and provides performance beneﬁts above both SVM and MKL.},
	language = {en},
	author = {Jaques, Natasha and Taylor, Sara and Sano, Akane and Picard, Rosalind},
	month = dec,
	year = {2015},
	pages = {7},
}

@article{jaques_multi-task_2016,
	title = {Multi-task {Learning} for {Predicting} {Health}, {Stress}, and {Happiness}},
	abstract = {Multi-task Learning (MTL) is applied to the problem of predicting next-day health, stress, and happiness using data from wearable sensors and smartphone logs. Three formulations of MTL are compared: i) Multi-task Multi-Kernel learning, which feeds information across tasks through kernel weights on feature types, ii) a Hierarchical Bayes model in which tasks share a common Dirichlet prior, and iii) Deep Neural Networks, which share several hidden layers but have ﬁnal layers unique to each task. We show that by using MTL to leverage data from across the population while still customizing a model for each person, we can account for individual differences, and obtain state-of-the-art performance on this dataset.},
	language = {en},
	author = {Jaques, Natasha and Taylor, Sara and Nosakhare, Ehimwenma and Sano, Akane and Picard, Rosalind},
	month = dec,
	year = {2016},
	pages = {5},
}

@inproceedings{jaques_predicting_2015,
	title = {Predicting students' happiness from physiology, phone, mobility, and behavioral data},
	doi = {10.1109/ACII.2015.7344575},
	abstract = {In order to model students' happiness, we apply machine learning methods to data collected from undergrad students monitored over the course of one month each. The data collected include physiological signals, location, smartphone logs, and survey responses to behavioral questions. Each day, participants reported their wellbeing on measures including stress, health, and happiness. Because of the relationship between happiness and depression, modeling happiness may help us to detect individuals who are at risk of depression and guide interventions to help them. We are also interested in how behavioral factors (such as sleep and social activity) affect happiness positively and negatively. A variety of machine learning and feature selection techniques are compared, including Gaussian Mixture Models and ensemble classification. We achieve 70\% classification accuracy of self-reported happiness on held-out test data.},
	booktitle = {2015 {International} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction} ({ACII})},
	author = {Jaques, Natasha and Taylor, Sara and Azaria, Asaph and Ghandeharioun, Asma and Sano, Akane and Picard, Rosalind},
	month = sep,
	year = {2015},
	note = {ISSN: 2156-8111},
	keywords = {Accelerometers, Atmospheric measurements, Energy measurement, Particle measurements, Physiology, Stress, Stress measurement, happiness, machine learning, wellbeing},
	pages = {222--228},
}

@inproceedings{umematsu_forecasting_2020,
	address = {Montreal, QC, Canada},
	title = {Forecasting stress, mood, and health from daytime physiology in office workers and students},
	isbn = {978-1-72811-990-8},
	url = {https://ieeexplore.ieee.org/document/9176706/},
	doi = {10.1109/EMBC44109.2020.9176706},
	abstract = {We examine the problem of forecasting tomorrow morning’s three self-reported levels (on scales from 0 to 100) of stressed-calm, sad-happy, and sick-healthy based on physiological data (skin conductance, skin temperature, and acceleration) from a sensor worn on the wrist from 10am-5pm today. We train automated forecasting regression algorithms using Random Forests and compare their performance over two sets of data: “workers” consisting of 490 days of weekday data from 39 employees at a high-tech company in Japan and “students” consisting of 3,841 days of weekday data from 201 New England USA college students. Mean absolute errors on held-out test data achieved 10.8, 13.5, and 14.4 for the estimated levels of mood, stress, and health respectively of ofﬁce workers, and 17.8, 20.3, and 20.4 for the mood, stress, and health respectively of students. Overall the two groups reported comparable stress and mood scores, while employees reported slightly poorer health, and engaged in signiﬁcantly lower levels of physical activity as measured by accelerometers. We further examine differences in population features and how systems trained on each population performed when tested on the other.},
	language = {en},
	urldate = {2021-11-08},
	booktitle = {2020 42nd {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} \& {Biology} {Society} ({EMBC})},
	publisher = {IEEE},
	author = {Umematsu, Terumi and Sano, Akane and Taylor, Sara and Tsujikawa, Masanori and Picard, Rosalind W.},
	month = jul,
	year = {2020},
	pages = {5953--5957},
}

@article{taylor_personalized_2020,
	title = {Personalized {Multitask} {Learning} for {Predicting} {Tomorrow}'s {Mood}, {Stress}, and {Health}},
	volume = {11},
	issn = {1949-3045},
	doi = {10.1109/TAFFC.2017.2784832},
	abstract = {While accurately predicting mood and wellbeing could have a number of important clinical benefits, traditional machine learning (ML) methods frequently yield low performance in this domain. We posit that this is because a one-size-fits-all machine learning model is inherently ill-suited to predicting outcomes like mood and stress, which vary greatly due to individual differences. Therefore, we employ Multitask Learning (MTL) techniques to train personalized ML models which are customized to the needs of each individual, but still leverage data from across the population. Three formulations of MTL are compared: i) MTL deep neural networks, which share several hidden layers but have final layers unique to each task; ii) Multi-task Multi-Kernel learning, which feeds information across tasks through kernel weights on feature types; and iii) a Hierarchical Bayesian model in which tasks share a common Dirichlet Process prior. We offer the code for this work in open source. These techniques are investigated in the context of predicting future mood, stress, and health using data collected from surveys, wearable sensors, smartphone logs, and the weather. Empirical results demonstrate that using MTL to account for individual differences provides large performance improvements over traditional machine learning methods and provides personalized, actionable insights.},
	number = {2},
	journal = {IEEE Transactions on Affective Computing},
	author = {Taylor, Sara and Jaques, Natasha and Nosakhare, Ehimwenma and Sano, Akane and Picard, Rosalind},
	month = apr,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Affective Computing},
	keywords = {Bayes methods, Data models, Meteorology, Mood, Mood prediction, Predictive models, Stress, deep neural networks, hierarchical Bayesian model, multi-kernel SVM, multitask learning},
	pages = {200--213},
}

@misc{noauthor_happimeterorg_nodate,
	title = {Happimeter.org {\textbar} {A} project of the {Center} for {Collective} {Intelligence} at {MIT}},
	url = {https://www.happimeter.org/},
	urldate = {2021-11-08},
}

@misc{noauthor_pattern_nodate,
	title = {Pattern - {Correlate}, {Health} {Diary}, {Mood}-{Tracker} - {Apps} on {Google} {Play}},
	url = {https://play.google.com/store/apps/details?id=com.pattern.health.diary},
	abstract = {Health Diary For Your Wellbeing. Discover What Works For A Healthier You.},
	language = {en},
	urldate = {2021-11-08},
}

@misc{noauthor_mood_nodate,
	title = {Mood {Tracker} {Journal}. {Mental} {Health}, {Depression} - {Apps} on {Google} {Play}},
	url = {https://play.google.com/store/apps/details?id=diary.questions.mood.tracker},
	abstract = {Mood Tracker Journal for mental health \& self care diary. Relieve depression},
	language = {en},
	urldate = {2021-11-08},
}

@misc{noauthor_moodily_nodate,
	title = {Moodily - {Mood} {Tracker}, {Depression} {Support} - {Apps} on {Google} {Play}},
	url = {https://play.google.com/store/apps/details?id=moodily.rohweller},
	abstract = {Track Your Daily Moods},
	language = {en},
	urldate = {2021-11-08},
}

@misc{noauthor_moodprism_nodate,
	title = {{MoodPrism} {Mental} health and wellbeing app},
	url = {https://www.moodprismapp.com},
	abstract = {MoodPrism is a smartphone app that helps you learn more about yourself by transforming daily mood reports into a colourful summary of your emotional health. You will receive feedback on your mental health and wellbeing, and the more you use the app, the more information you will receive.},
	language = {en-AU},
	urldate = {2021-11-08},
	journal = {MoodPrism Mental health and wellbeing app},
}

@misc{noauthor_moodpanda_nodate,
	title = {{MoodPanda} - {Your} supportive mood diary},
	url = {https://www.moodpanda.com/},
	abstract = {Mobile \& web app for mood-tracking. Analyse your mood on graphs and calendars; get support and advice from the MoodPanda community},
	language = {en},
	urldate = {2021-11-08},
	journal = {MoodPanda.com},
}

@misc{noauthor_daylio_nodate,
	title = {Daylio - {Journal}, {Diary} and {Mood} {Tracker}},
	url = {https://daylio.net/},
	abstract = {Self-Care Bullet Journal with Goals Mood Diary \& Happiness Tracker},
	language = {en-US},
	urldate = {2021-11-08},
	journal = {Daylio},
}

@misc{noauthor_mood_nodate-1,
	title = {Mood {Patterns} - {Mood} tracker \& diary with privacy},
	url = {https://www.moodpatterns.info/},
	abstract = {Gain insights into your feelings},
	language = {en-US},
	urldate = {2021-11-08},
}

@misc{noauthor_features_nodate,
	title = {Features},
	url = {https://www.correlate.com/features/},
	abstract = {Correlate give you the power and flexibility of more complex apps in an easy to use app. Browse through the many powerful features.},
	language = {en-US},
	urldate = {2021-11-08},
	journal = {Correlate.com},
}

@inproceedings{kang_identifying_2016,
	title = {Identifying depressive users in {Twitter} using multimodal analysis},
	doi = {10.1109/BIGCOMP.2016.7425918},
	abstract = {Recently, many efforts have been spent on observing individual's psychological states through analyzing users' social activities on SNS. In this paper, we propose a novel method for identifying the users with depressive moods by analyzing their daily tweets for a long period of time. Then, for more accurately understand their tweets, we exploit all media types of tweets, i.e., images and emoticons as well as texts, thus develop a multimodal method for analyzing them. In the proposed method, three single-modal analyses are first performed for extract the hidden users' moods from text, emoticon, and images: a learning based text analysis, a word-based emoticon analysis, and a SVM based image classifier. Thereafter, the extracted moods from the respective analyses are integrated into a mood and again aggregated per a day, which allows for continuous monitoring of user's mood trends. To assess the validity of the proposed method, two types of experiments were performed: 1) the proposed multimodal analysis was tested with a number of tweets, and its performance was compared to SentiStrength; 2) it was applied to classify 45 users' mental states as depressive and non-depressive ones. Then, the results demonstrated that the proposed method outperforms the baseline, and it is effective in finding depressive moods for users.},
	booktitle = {2016 {International} {Conference} on {Big} {Data} and {Smart} {Computing} ({BigComp})},
	author = {Kang, Keumhee and Yoon, Chanhee and Kim, Eun Yi},
	month = jan,
	year = {2016},
	note = {ISSN: 2375-9356},
	keywords = {Correlation, Internet, Media, Mood, Mood prediction, Sentiment analysis, Twitter, Visualization},
	pages = {231--238},
}

@article{liu_multimodal_2020,
	title = {Multimodal {Privacy}-preserving {Mood} {Prediction} from {Mobile} {Data}: {A} {Preliminary} {Study}},
	shorttitle = {Multimodal {Privacy}-preserving {Mood} {Prediction} from {Mobile} {Data}},
	url = {http://arxiv.org/abs/2012.02359},
	abstract = {Mental health conditions remain under-diagnosed even in countries with common access to advanced medical care. The ability to accurately and efficiently predict mood from easily collectible data has several important implications towards the early detection and intervention of mental health disorders. One promising data source to help monitor human behavior is from daily smartphone usage. However, care must be taken to summarize behaviors without identifying the user through personal (e.g., personally identifiable information) or protected attributes (e.g., race, gender). In this paper, we study behavioral markers or daily mood using a recent dataset of mobile behaviors from high-risk adolescent populations. Using computational models, we find that multimodal modeling of both text and app usage features is highly predictive of daily mood over each modality alone. Furthermore, we evaluate approaches that reliably obfuscate user identity while remaining predictive of daily mood. By combining multimodal representations with privacy-preserving learning, we are able to push forward the performance-privacy frontier as compared to unimodal approaches.},
	urldate = {2021-11-04},
	journal = {arXiv:2012.02359 [cs, stat]},
	author = {Liu, Terrance and Liang, Paul Pu and Muszynski, Michal and Ishii, Ryo and Brent, David and Auerbach, Randy and Allen, Nicholas and Morency, Louis-Philippe},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.02359},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Applications},
}

@article{harper_bayesian_2020,
	title = {A {Bayesian} {Deep} {Learning} {Framework} for {End}-{To}-{End} {Prediction} of {Emotion} from {Heartbeat}},
	issn = {1949-3045, 2371-9850},
	url = {http://arxiv.org/abs/1902.03043},
	doi = {10.1109/TAFFC.2020.2981610},
	abstract = {Automatic prediction of emotion promises to revolutionise human-computer interaction. Recent trends involve fusion of multiple data modalities - audio, visual, and physiological - to classify emotional state. However, in practice, collection of physiological data `in the wild' is currently limited to heartbeat time series of the kind generated by affordable wearable heart monitors. Furthermore, real-world applications of emotion prediction often require some measure of uncertainty over model output, in order to inform downstream decision-making. We present here an end-to-end deep learning model for classifying emotional valence from unimodal heartbeat time series. We further propose a Bayesian framework for modelling uncertainty over these valence predictions, and describe a probabilistic procedure for choosing to accept or reject model output according to the intended application. We benchmarked our framework against two established datasets and achieved peak classification accuracy of 90\%. These results lay the foundation for applications of affective computing in real-world domains such as healthcare, where a high premium is placed on non-invasive collection of data, and predictive certainty.},
	urldate = {2021-11-04},
	journal = {IEEE Transactions on Affective Computing},
	author = {Harper, Ross and Southern, Joshua},
	year = {2020},
	note = {arXiv: 1902.03043},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1--1},
}

@inproceedings{jaques_multimodal_2017,
	title = {Multimodal autoencoder: {A} deep learning approach to filling in missing sensor data and enabling better mood prediction},
	shorttitle = {Multimodal autoencoder},
	doi = {10.1109/ACII.2017.8273601},
	abstract = {To accomplish forecasting of mood in real-world situations, affective computing systems need to collect and learn from multimodal data collected over weeks or months of daily use. Such systems are likely to encounter frequent data loss, e.g. when a phone loses location access, or when a sensor is recharging. Lost data can handicap classifiers trained with all modalities present in the data. This paper describes a new technique for handling missing multimodal data using a specialized denoising autoencoder: the Multimodal Autoencoder (MMAE). Empirical results from over 200 participants and 5500 days of data demonstrate that the MMAE is able to predict the feature values from multiple missing modalities more accurately than reconstruction methods such as principal components analysis (PCA). We discuss several practical benefits of the MMAE's encoding and show that it can provide robust mood prediction even when up to three quarters of the data sources are lost.},
	booktitle = {2017 {Seventh} {International} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction} ({ACII})},
	author = {Jaques, Natasha and Taylor, Sara and Sano, Akane and Picard, Rosalind},
	month = oct,
	year = {2017},
	note = {ISSN: 2156-8111},
	keywords = {Data models, Mood, Noise measurement, Noise reduction, Principal component analysis, Stress, Training},
	pages = {202--208},
}

@article{tov_subjective_2013,
	title = {Subjective {Well}-being},
	url = {https://ink.library.smu.edu.sg/soss_research/1395},
	doi = {10.1002/9781118339893.wbeccp518},
	journal = {Encyclopedia of Cross-Cultural Psychology},
	author = {TOV, William and DIENER, Ed},
	month = jan,
	year = {2013},
}
